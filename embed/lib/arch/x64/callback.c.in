// Code for a callback trampoline.
static uint8_t cbtrampoline[16] = {
   0x4c, 0x8b, 0x15, 0x09, 0x00, 0x00, 0x00, // mov    0x9(%rip),%r10
   0xff, 0x25, 0x0b, 0x00, 0x00, 0x00,       // jmp    *0xb(%rip)
   0x0f, 0x01f, 0x00,                        // nop
};

struct CallbackEntry {
    uint8_t code[16];
    uint64_t target;
    uint64_t trampoline;
};

static struct CallbackEntry* cbentries_alias;
static struct CallbackEntry* cbentries_box;

static bool
cbinit(struct LFIContext* ctx)
{
    int fd = memfd_create("", 0);
    if (fd < 0)
        return false;
    size_t size = MAXCALLBACKS * sizeof(struct CallbackEntry);
    int r = ftruncate(fd, size);
    if (r < 0)
        goto err;
    // Map callback entries outside the sandbox as read/write.
    void* aliasmap = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
    if (aliasmap == (void*) -1)
        goto err;
    cbentries_alias = (struct CallbackEntry*) aliasmap;
    // Fill in the code for each entry.
    for (size_t i = 0; i < MAXCALLBACKS; i++) {
        memcpy(&cbentries_alias[i].code, &cbtrampoline[0], sizeof(cbentries_alias[i].code));
    }
    struct HostFile* hf = lfi->lfi_host_fdopen(fd);
    assert(hf);
    // Share the mapping inside the sandbox as read/exec.
    lfiptr_t boxmap = lfi->lfi_as_mapany(lfi->lfi_ctx_as(ctx), size, PROT_READ | PROT_EXEC, MAP_SHARED, hf, 0);
    if (boxmap == (lfiptr_t) -1)
        goto err1;
    cbentries_box = (struct CallbackEntry*) boxmap;
    return true;
err1:
    munmap(aliasmap, size);
err:
    close(fd);
    return false;
}

void*
{{ .lib }}_register_cb(void* fn, size_t stackframe)
{
    assert(fn);
    assert(cbfind(fn) == -1 && "fn is already registered as a callback");

    ssize_t slot = cbfreeslot();
    if (slot == -1)
        return NULL;

    // TODO: support non-zero stackframes: create a trampoline for 'stackframe'
    // if it does not exist
    if (stackframe != 0)
        return NULL;

    // write 'fn' into the 'target' field for the chosen slot.
    __atomic_store_n(&cbentries_alias[slot].target, (uint64_t) fn, __ATOMIC_SEQ_CST);
    // write the trampoline into the 'trampoline' field for the chosen slot
    __atomic_store_n(&cbentries_alias[slot].trampoline, (uint64_t) {{.lib}}_cbtrampoline, __ATOMIC_SEQ_CST);

    // Mark the slot as allocated.
    callbacks[slot] = fn;

    return &cbentries_box[slot].code[0];
}

void
{{ .lib }}_unregister_cb(void* fn)
{
    ssize_t slot = cbfind(fn);
    if (slot == -1)
        return;
    callbacks[slot] = NULL;
    __atomic_store_n(&cbentries_alias[slot].target, 0, __ATOMIC_SEQ_CST);
    __atomic_store_n(&cbentries_alias[slot].trampoline, 0, __ATOMIC_SEQ_CST);
}

void*
{{ .lib }}_cb_addr(void* cb)
{
    return (void*)*(uintptr_t*)((char*)cb + sizeof(cbtrampoline));
}

void*
{{ .lib }}_addr(void* sym)
{
    const size_t trampoline_size = 16;
    size_t diff = (uintptr_t)sym - (uintptr_t)&__lfi_trampolines;
    return diff % trampoline_size == 0 && (diff / trampoline_size) < __lfi_trampotable_size ? __lfi_trampotable[diff / trampoline_size] : NULL;
}
